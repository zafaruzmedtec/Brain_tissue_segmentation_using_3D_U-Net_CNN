{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from unet3d.data import write_data_to_file, open_data_file\n",
    "from unet3d.generator import get_training_and_validation_generators\n",
    "from unet3d.model import isensee2017_model\n",
    "from unet3d.training import load_old_model, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config[\"image_shape\"] = (256, 128, 256)  # This determines what shape the images will be cropped/resampled to.\n",
    "config[\"patch_shape\"] = (64,64,64)  # switch to None to train on the whole image\n",
    "config[\"labels\"] = (0, 1, 2, 3)  # the label numbers on the input image\n",
    "config[\"n_base_filters\"] = 16\n",
    "config[\"n_labels\"] = len(config[\"labels\"])\n",
    "config[\"all_modalities\"] = [\"t1\"]\n",
    "config[\"training_modalities\"] = config[\"all_modalities\"]  # change this if you want to only use some of the modalities\n",
    "config[\"nb_channels\"] = len(config[\"training_modalities\"])\n",
    "if \"patch_shape\" in config and config[\"patch_shape\"] is not None:\n",
    "    config[\"input_shape\"] = tuple([config[\"nb_channels\"]] + list(config[\"patch_shape\"]))\n",
    "else:\n",
    "    config[\"input_shape\"] = tuple([config[\"nb_channels\"]] + list(config[\"image_shape\"]))\n",
    "config[\"truth_channel\"] = config[\"nb_channels\"]\n",
    "config[\"deconvolution\"] = True  # if False, will use upsampling instead of deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"batch_size\"] = 2\n",
    "config[\"validation_batch_size\"] = 4\n",
    "config[\"n_epochs\"] = 500  # cutoff the training after this many epochs\n",
    "config[\"patience\"] = 10  # learning rate will be reduced after this many epochs if the validation loss is not improving\n",
    "config[\"early_stop\"] = 50  # training will be stopped after this many epochs without the validation loss improving\n",
    "config[\"initial_learning_rate\"] = 5e-4\n",
    "config[\"learning_rate_drop\"] = 0.5  # factor by which the learning rate will be reduced\n",
    "config[\"validation_split\"] = 0.8  # portion of the data that will be used for training\n",
    "config[\"flip\"] = False  # augments the data by randomly flipping an axis during\n",
    "config[\"permute\"] = True  # data shape must be a cube. Augments the data by permuting in various directions\n",
    "config[\"distort\"] = False  # switch to None if you want no distortion\n",
    "config[\"augment\"] = config[\"flip\"] or config[\"distort\"]\n",
    "config[\"validation_patch_overlap\"] = 0  # if > 0, during training, validation patches will be overlapping\n",
    "config[\"training_patch_start_offset\"] = (16, 16, 16)  # randomly offset the first patch index by up to this offset\n",
    "config[\"skip_blank\"] = True  # if True, then patches without any target will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data_file\"] = os.path.abspath(\"data.h5\")\n",
    "config[\"model_file\"] = os.path.abspath(\"model.h5\")\n",
    "config[\"training_file\"] = os.path.abspath(\"train.pkl\")\n",
    "config[\"validation_file\"] = os.path.abspath(\"val.pkl\")\n",
    "config[\"overwrite\"] = True  # If True, will replace previous files. If False, will use previously written files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_training_data_files(return_subject_ids=False):\n",
    "    training_data_files = list()\n",
    "    subject_ids = list()\n",
    "    \n",
    "    train_img = glob.glob(os.path.join('.', \"Data\", 'Train','img','*'))\n",
    "    train_seg = glob.glob(os.path.join('.', \"Data\", 'Train','seg','*'))\n",
    "    \n",
    "    for i in range(len(train_img)):\n",
    "        subject_ids.append(os.path.basename(train_img[i]))\n",
    "        subject_files = list()\n",
    "        subject_files.append(os.path.join(train_img[i]))\n",
    "        subject_files.append(os.path.join(train_seg[i]))\n",
    "        training_data_files.append(tuple(subject_files))\n",
    "        \n",
    "        \n",
    "    val_img = glob.glob(os.path.join('.', \"Data\", 'Validation','img','*'))\n",
    "    val_seg = glob.glob(os.path.join('.', \"Data\", 'Validation','seg','*'))\n",
    "    \n",
    "    for i in range(len(val_img)):\n",
    "        subject_ids.append(os.path.basename(val_img[i]))\n",
    "        subject_files = list()\n",
    "        subject_files.append(os.path.join(val_img[i]))\n",
    "        subject_files.append(os.path.join(val_seg[i]))\n",
    "        training_data_files.append(tuple(subject_files))\n",
    "        \n",
    "        \n",
    "    if return_subject_ids:\n",
    "        return training_data_files, subject_ids\n",
    "    else:\n",
    "        return training_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite=True\n",
    "# convert input images into an hdf5 file\n",
    "if overwrite or not os.path.exists(config[\"data_file\"]):\n",
    "    training_files, subject_ids = fetch_training_data_files(return_subject_ids=True)\n",
    "    write_data_to_file(training_files, config[\"data_file\"], image_shape=config[\"image_shape\"],\n",
    "                       subject_ids=subject_ids)\n",
    "    \n",
    "    \n",
    "data_file_opened = open_data_file(config[\"data_file\"])\n",
    "if not overwrite and os.path.exists(config[\"model_file\"]):\n",
    "    model = load_old_model(config[\"model_file\"])\n",
    "else:\n",
    "# instantiate new model\n",
    "    model = isensee2017_model(input_shape=config[\"input_shape\"], n_labels=config[\"n_labels\"],\n",
    "                          initial_learning_rate=config[\"initial_learning_rate\"],\n",
    "                          n_base_filters=config[\"n_base_filters\"])\n",
    "\n",
    "# Loading \"twice\" in str output only, triple checked, actually loaded once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator, validation_generator, n_train_steps, n_validation_steps = get_training_and_validation_generators(\n",
    "    data_file_opened,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    data_split=config[\"validation_split\"],\n",
    "    overwrite=overwrite,\n",
    "    validation_keys_file=config[\"validation_file\"],\n",
    "    training_keys_file=config[\"training_file\"],\n",
    "    n_labels=config[\"n_labels\"],\n",
    "    labels=config[\"labels\"],\n",
    "    patch_shape=config[\"patch_shape\"],\n",
    "    validation_batch_size=config[\"validation_batch_size\"],\n",
    "    validation_patch_overlap=config[\"validation_patch_overlap\"],\n",
    "    training_patch_start_offset=config[\"training_patch_start_offset\"],\n",
    "    permute=config[\"permute\"],\n",
    "    augment=config[\"augment\"],\n",
    "    skip_blank=config[\"skip_blank\"],\n",
    "    augment_flip=config[\"flip\"],\n",
    "    augment_distortion_factor=config[\"distort\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model=model,\n",
    "            model_file=config[\"model_file\"],\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=n_train_steps,\n",
    "            validation_steps=n_validation_steps,\n",
    "            initial_learning_rate=config[\"initial_learning_rate\"],\n",
    "            learning_rate_drop=config[\"learning_rate_drop\"],\n",
    "            learning_rate_patience=config[\"patience\"],\n",
    "            early_stopping_patience=config[\"early_stop\"],\n",
    "            n_epochs=config[\"n_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_opened.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
